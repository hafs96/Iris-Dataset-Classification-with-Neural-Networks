{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"\" ><img src=\"./iris.jpeg\" style=\"float:left; max-width: 200px; height: 145px; \"/></a> \n",
    "<a href=\"https://github.com/hafs96/Iris-Dataset-Classification-with-Neural-Networks.git\" ><img src=\"./git.jpeg\" style=\"float:right; max-width: 50px; display: inline \" /></a>\n",
    "</center>\n",
    "<div id=\"report\" style=\"text-align: center; padding: 10px; background-color:rgb(17, 16, 16); border-radius: 10px; box-shadow: 0 4px 8px rgba(0, 0, 0, 0.1); \">\n",
    "  <h1 style=\"font-size: 2.2em; font-family: 'Georgia', serif; color:rgb(247, 239, 248); margin-bottom: 10px;\">\n",
    "    Iris Neural Network Experiment\n",
    "  </h1>\n",
    "  <p style=\"font-size: 1.1em; font-family: 'Arial', sans-serif; color:rgb(227, 231, 235); margin: 5px 0;\">\n",
    "    Prepared by :  ZIAN Hafsa\n",
    "  </p>\n",
    "  <div style=\"margin-top: 15px; padding: 5px; background-color: #ecf0f1; border-left: 4px solid rgb(127, 50, 129); border-radius: 8px; display: inline-block; max-width: 60%;\">\n",
    "    <p style=\"font-size: 1.1em; font-family: 'Arial', sans-serif; color:rgb(16, 17, 17); margin: 0; font-style: italic;\">\n",
    "      Supervised by : \n",
    "      <span style=\"font-weight: bold; color:rgb(129, 27, 116);\">Prof. EL YAZIDI Youness</span>\n",
    "    </p>\n",
    "  </div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div id=\"report\" style=\"text-align: center; padding: 20px; \">\n",
    "  <h1 style=\"font-size: 2em; font-family: 'Georgia', serif; color: #fff; background-color: #333; padding: 10px;\">  \n",
    "  &#9733; Report Content   &#9733; </h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. <a href=\"#intro\">Introduction</a><br>\n",
    "\n",
    "2. <a href=\"#dataset\">Iris Dataset</a><br>\n",
    "\n",
    "3. <a href=\"#methodology\">Methodology</a><br>\n",
    "   3.1 <a href=\"#data_preparation\">Data Preparation</a><br>\n",
    "   3.2 <a href=\"#splitting\">Splitting Training, Validation, and Testing Sets</a><br>\n",
    "\n",
    "4. <a href=\"#activation_functions\">Activation Function Comparison</a><br>\n",
    "   4.1 <a href=\"#hidden_layer\">Hidden Layer Activation Functions</a><br>\n",
    "   4.2 <a href=\"#output_layer\">Output Layer Activation Functions</a><br>\n",
    "\n",
    "5. <a href=\"#loss_functions\">Loss Function Evaluation</a><br>\n",
    "   5.1 <a href=\"#cross_entropy\">Cross-Entropy Loss</a><br>\n",
    "   5.2 <a href=\"#hinge_loss\">Hinge Loss</a><br>\n",
    "\n",
    "6. <a href=\"#optimizers\">Optimizer Selection</a><br>\n",
    "   6.1 <a href=\"#rmsprop\">RMSProp Optimizer</a><br>\n",
    "   6.2 <a href=\"#adam\">Adam Optimizer</a><br>\n",
    "\n",
    "7. <a href=\"#evaluation_metrics\">Evaluation Metrics</a><br>\n",
    "\n",
    "8. <a href=\"#cross_validation\">Cross-Validation Analysis</a><br>\n",
    "\n",
    "9. <a href=\"#results\">Results and Discussion</a><br>\n",
    "\n",
    "10. <a href=\"#conclusion\">Conclusion and Future Work</a><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Introduction<span id=\"intro\"></span>\n",
    "\n",
    "\n",
    "## Problem Statement\n",
    "Classification is a fundamental task in machine learning, and the Iris dataset is a well-known benchmark for testing classification models. It consists of three flower species: Iris Setosa, Iris Versicolor, and Iris Virginica, each represented by four numerical features: sepal length, sepal width, petal length, and petal width.\n",
    "\n",
    "In this study, we aim to develop a neural network with a single hidden layer to classify these species accurately. We will experiment with different activation functions, loss functions, and optimizers to determine the most effective configuration.\n",
    "\n",
    "## Study Objectives\n",
    "The primary goals of this project are to:\n",
    "\n",
    "- Build a feedforward neural network for classifying Iris flowers.\n",
    "- Compare various activation functions (Sigmoid, Tanh, ReLU) in the hidden layer.\n",
    "- Evaluate the performance of Softmax vs. Sigmoid in the output layer.\n",
    "- Test different loss functions (Cross-Entropy and Hinge Loss) and analyze their impact.\n",
    "- Compare the effectiveness of RMSProp and Adam optimizers.\n",
    "- Assess the model using appropriate evaluation metrics and cross-validation techniques.\n",
    "- Improve model generalization by incorporating a validation set in addition to training and testing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Iris Dataset <span id=\"dataset\"></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## imports and preliminaries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the Iris Dataset\n",
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Methodology<span id=\"methodology\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Data Preparation <span id=\"data_preparation\"></span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.data, data.target\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "Y = encoder.fit_transform(y.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Splitting Training, Validation, and Testing Sets <span id=\"splitting\"></span>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Split the dataset into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris data: \n",
      "[[5.1 3.5 1.4 0.2]\n",
      " [4.9 3.  1.4 0.2]\n",
      " [4.7 3.2 1.3 0.2]\n",
      " [4.6 3.1 1.5 0.2]\n",
      " [5.  3.6 1.4 0.2]\n",
      " [5.4 3.9 1.7 0.4]\n",
      " [4.6 3.4 1.4 0.3]\n",
      " [5.  3.4 1.5 0.2]\n",
      " [4.4 2.9 1.4 0.2]\n",
      " [4.9 3.1 1.5 0.1]]\n"
     ]
    }
   ],
   "source": [
    "print('Iris data: ')\n",
    "print(data.data[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Activation Function Comparison <span id=\"activation_functions\"></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Hidden Layer Activation Functions <span id=\"hidden_layer\"></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_activations = ['sigmoid', 'tanh', 'relu']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Output Layer Activation Functions<span id=\"output_layer\"></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_activations = ['sigmoid', 'softmax']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for hidden activation sigmoid and output activation sigmoid: 0.00%\n",
      "Accuracy for hidden activation sigmoid and output activation softmax: 0.00%\n",
      "Accuracy for hidden activation tanh and output activation sigmoid: 0.00%\n",
      "Accuracy for hidden activation tanh and output activation softmax: 0.00%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.metrics import Accuracy\n",
    "# Prepare for plotting the loss decay\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for hidden_activation in hidden_activations:\n",
    "    for output_activation in output_activations:\n",
    "        # Build the model\n",
    "        model = Sequential()\n",
    "        model.add(Dense(10, input_dim=X.shape[1], activation=hidden_activation))  # Hidden layer\n",
    "        model.add(Dense(3, activation=output_activation))  # Output layer (3 classes)\n",
    "\n",
    "        # Compile the model\n",
    "        model.compile(optimizer=SGD(), loss='mean_squared_error', metrics=[Accuracy()])\n",
    "\n",
    "        # Train the model\n",
    "        history = model.fit(X_train, Y_train, epochs=100, batch_size=10, verbose=0, validation_data=(X_test, Y_test))\n",
    "\n",
    "        # Plot the loss\n",
    "        plt.plot(history.history['loss'], label=f'{hidden_activation}-{output_activation}')\n",
    "\n",
    "        # Evaluate the model\n",
    "        _, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "        print(f\"Accuracy for hidden activation {hidden_activation} and output activation {output_activation}: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Customize the plot\n",
    "plt.title(\"Loss decay for different activation functions\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD, RMSprop, Adam\n",
    "\n",
    "# Charger et préparer les données\n",
    "data = load_iris()\n",
    "X, y = data.data, data.target\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "Y = encoder.fit_transform(y.reshape(-1, 1))\n",
    "\n",
    "# Diviser les données en ensemble d'entraînement, validation et test\n",
    "X_train, X_val_test, Y_train, Y_val_test = train_test_split(X, Y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, Y_val, Y_test = train_test_split(X_val_test, Y_val_test, test_size=0.5, random_state=42)\n",
    "\n",
    "# Définition des combinaisons de fonctions d'activation et de coût\n",
    "hidden_activation = 'relu'\n",
    "output_activation = 'softmax'\n",
    "loss_functions = ['categorical_crossentropy', 'hinge']\n",
    "optimizers = {'RMSprop': RMSprop(learning_rate=0.01), 'Adam': Adam(learning_rate=0.01)}\n",
    "\n",
    "# Stocker les pertes et précisions\n",
    "losses = {}\n",
    "accuracies = {}\n",
    "\n",
    "# Fonction pour créer et entraîner le modèle\n",
    "def train_model(loss_function, optimizer_name):\n",
    "    model = Sequential([\n",
    "        Dense(10, activation=hidden_activation, input_shape=(4,)),\n",
    "        Dense(3, activation=output_activation)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=optimizers[optimizer_name], loss=loss_function, metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train, Y_train, epochs=100, verbose=0, validation_data=(X_val, Y_val))\n",
    "    \n",
    "    test_loss, test_acc = model.evaluate(X_test, Y_test, verbose=0)\n",
    "    losses[f\"{loss_function}_{optimizer_name}\"] = history.history['loss']\n",
    "    accuracies[f\"{loss_function}_{optimizer_name}\"] = test_acc\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Entraînement des modèles pour différentes fonctions de coût et optimisateurs\n",
    "for loss_function in loss_functions:\n",
    "    for optimizer_name in optimizers.keys():\n",
    "        train_model(loss_function, optimizer_name)\n",
    "\n",
    "# Tracer les courbes de perte\n",
    "plt.figure(figsize=(10, 6))\n",
    "for key, loss in losses.items():\n",
    "    plt.plot(loss, label=key)\n",
    "plt.xlabel(\"Époque\")\n",
    "plt.ylabel(\"Perte\")\n",
    "plt.title(\"Évolution de la perte pour différentes fonctions de coût et optimisateurs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Affichage des précisions\n",
    "for key, acc in accuracies.items():\n",
    "    print(f\"Modèle {key} - Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Validation croisée\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,), activation='relu', solver='adam', max_iter=1000, random_state=42)\n",
    "scores = cross_val_score(mlp, X, y, cv=5)\n",
    "print(\"Scores de validation croisée:\", scores)\n",
    "print(\"Moyenne des scores:\", scores.mean())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
